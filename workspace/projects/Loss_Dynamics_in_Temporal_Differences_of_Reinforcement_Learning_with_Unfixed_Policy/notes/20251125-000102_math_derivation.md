## I. On-policy + 固定策略 + 函数逼近 + 自举  
（可对应论文 3.1 小节：固定策略 on-policy TD(0) 的 DMFT 复现）

### 问题设定（固定策略 on-policy TD(0)）

**环境与策略.**  
沿用 [PAPER 1, Sec.2] 的设定：给定一个有限或可数状态空间 \(\mathcal S\)，固定策略 \(\pi(a\mid s)\)（行为策略 = 评估策略），其在环境转移 \(P(s' \mid s,a)\) 下诱导出一族长度为 \(T\) 的轨迹分布
\[
p(\tau) = p(s_1,\dots,s_T),\quad \tau=\{s_t\}_{t=1}^T,
\]
每个 episode 独立同分布抽取。

**特征与线性值函数.**  
每个状态 \(s\) 用 \(N\)-维特征向量表示
\[
\bm\psi(s)\in\mathbb R^N,
\]
预测值函数
\[
\hat V(s) = \bm\psi(s)^\top \bm w,\quad \bm w\in\mathbb R^N.
\]
奖励函数设为线性可表示（[PAPER 1, App.~A]）：
\[
R(s) = \bm w_R^\top \bm\psi(s),
\]
真实值函数也可在同一特征子空间中表示
\[
V(s) = \bm w_{TD}^\top \bm\psi(s).
\]

**时间与 episode 记号.**  
- 迭代 index：\(n=0,1,2,\dots\)。  
- 第 \(n\) 次迭代抽到的第 \(\mu\) 条轨迹记为 \(\tau_n^\mu = \{s_n^\mu(t)\}_{t=1}^T\)，\(\mu=1,\dots,B\)；  
- 对应特征 \(\bm\psi_n^\mu(t) = \bm\psi(s_n^\mu(t))\)。

**TD(0) 误差与更新.**  
在 [PAPER 1, Sec.2.1] 的 online batch 设置下，每次迭代重采样 \(B\) 条新 episode，并用全 episode 的 TD(0) 半梯度更新：
\[
\Delta_n^\mu(t) = R\big(s_n^\mu(t)\big)
+ \gamma \hat V_n\big(s_n^\mu(t+1)\big)
- \hat V_n\big(s_n^\mu(t)\big)
\]
\[
= \bm w_R^\top\bm\psi_n^\mu(t)
+ \gamma\,\bm w_n^\top\bm\psi_n^\mu(t+1)
- \bm w_n^\top\bm\psi_n^\mu(t),
\]
\[
\bm w_{n+1}
= \bm w_n
+ \frac{\eta}{TB}\sum_{\mu=1}^B\sum_{t=1}^T
\Delta_n^\mu(t)\,\bm\psi_n^\mu(t),
\tag{1.1}
\]
其中 \(\eta\) 是学习率（为简洁起见暂时取常数）。这是标准的基于 episode mini-batch 的 on-policy TD(0) 半梯度更新。

**我们关心的损失.**  
按 [PAPER 1, Prop.2.1]，学习曲线可以用对真实值函数的均方误差表示：
\[
\mathcal L_n
= \big\langle \big(V(s)-\hat V_n(s)\big)^2 \big\rangle_{s\sim d_\pi},
\]
在高维极限下可写成
\[
\mathcal L_n
= \frac{1}{N}\mathrm{Tr}\big(\bar{\bm\Sigma}\,\bm M_n\big), \quad
\bm M_n = \Big\langle
(\bm w_n-\bm w_{TD})(\bm w_n-\bm w_{TD})^\top
\Big\rangle,
\tag{1.2}
\]
其中 \(\bar{\bm\Sigma}\) 是时间平均的特征协方差（下文定义），\(\langle\cdot\rangle\) 为对轨迹随机性的平均。

下面沿 [PAPER 1, App.~A]，系统走完

> 问题设定 → 高斯等价 → 矩生成函数 → 序参量 → 鞍点 → HS 变换 → 闭合

的整个流程。

---

### 高斯等价与矩生成函数

#### 高斯等价假设（Gaussian Equivalence）

**定义时间相关协方差.**  
在固定策略 \(\pi\) 诱导的轨迹分布 \(p(\tau)\) 下，定义特征的一阶、二阶时间相关统计量（[PAPER 1, Sec.3.2]）：
\[
\bm\mu(t) = \big\langle \bm\psi(s_t)\big\rangle_{\tau\sim p(\tau)},\quad
\bm\Sigma(t,t') = 
\Big\langle 
\bm\psi(s_t)\bm\psi(s_{t'})^\top
\Big\rangle_{\tau\sim p(\tau)}.
\tag{1.3}
\]

**高斯等价假设.** （[PAPER 1, Geq Conjecture]）  
在高维极限 \(N\to\infty\) 下，真实特征序列 \(\{\bm\psi(s_t)\}\) 诱导的 TD 学习曲线，可以用一个**时间相关高斯过程**替代：
\[
\bm\psi_G(t) \sim \mathcal N\big(\bm\mu(t),
\bm\Sigma(t,t')-\bm\mu(t)\bm\mu(t')^\top\big),
\tag{1.4}
\]
即用匹配均值与协方差的高斯序列代替真实特征序列。

> 【说明】  
> - 这里“利用高斯等价假设”：只保留特征分布的前两阶矩，高阶 cumulant 被忽略。  
> - 其合理性在 [PAPER 1, Sec.3.3, App.~C] 中通过大量数值实验验证，且与监督学习中的 universality 结果一致。  
> - 我们在后文仅在此假设下做平均。

为简洁，下面默认特征零均值
\(\bm\mu(t)=0\)；非零均值的修正参见 [PAPER 1, App.~A.3]。

#### 矩生成函数

我们关心 \(\{\bm w_n\}\) 的统计规律。引入路径积分形式的矩生成函数（[PAPER 1, App.~A.1]）：
\[
Z[\{\bm j_n\}]
=
\mathbb E_{\{\bm\psi_n^\mu(t)\}}
\exp\Big(i\sum_{n\ge 0}\bm j_n\cdot\bm w_n\Big),
\tag{1.5}
\]
其中平均是对所有 episode 的随机特征序列的平均。

**引入动力学约束.**  
利用 Dirac delta 约束更新式 (1.1) 和 TD 误差定义：

- 对权重更新：
\[
\delta\!\left(
\bm w_{n+1}-\bm w_n - \frac{\eta}{TB}
\sum_{\mu t}\Delta_n^\mu(t)\bm\psi_n^\mu(t)
\right),
\]
- 对 TD 误差：
\[
\delta\!\left(
\Delta_n^\mu(t) - \bm w_R^\top\bm\psi_n^\mu(t)
- \gamma\bm w_n^\top\bm\psi_n^\mu(t+1)
+ \bm w_n^\top\bm\psi_n^\mu(t)
\right).
\]

用傅里叶表示
\(\delta(z) = \int \frac{d\hat z}{2\pi}\,e^{i\hat z z}\)，
对每个 \(n,\mu,t\) 和每个 \(n\) 引入共轭变量
\(\hat\Delta_n^\mu(t)\)、\(\hat{\bm w}_n\)，得到（略去常数因子）：
\[
\begin{aligned}
Z[\{\bm j_n\}]
&=
\mathbb E_{\{\bm\psi\}}
\int \prod_n d\bm w_n d\hat{\bm w}_n
\prod_{n,\mu,t} d\Delta_n^\mu(t)d\hat\Delta_n^\mu(t)
\\
&\quad\times
\exp\Bigg\{
i\sum_n\bm j_n\cdot\bm w_n
+ i\sum_n\hat{\bm w}_n\cdot\Big(
\bm w_{n+1}-\bm w_n
-\frac{\eta}{TB}\sum_{\mu t}\Delta_n^\mu(t)\bm\psi_n^\mu(t)\Big)
\\
&\qquad
+ i\sum_{n,\mu,t}\hat\Delta_n^\mu(t)
\Big(\Delta_n^\mu(t)
- \bm w_R^\top\bm\psi_n^\mu(t)
- \gamma\bm w_n^\top\bm\psi_n^\mu(t+1)
+ \bm w_n^\top\bm\psi_n^\mu(t)
\Big)
\Bigg\}.
\end{aligned}
\tag{1.6}
\]

#### 对高斯特征求平均

现在“利用高斯等价假设”，将 \(\bm\psi\) 看成零均值高斯向量，对其做高斯积分。

每个 exponent 中包含形如
\[
\exp\Big(
 - i \sum_{n\mu tt'}
\Big[
\frac{\eta}{TB}\hat{\bm w}_n \Delta_n^\mu(t)
+ \hat\Delta_n^\mu(t)(\bm w_R - \bm w_n)
+ \gamma\hat\Delta_n^\mu(t-1)\bm w_n
\Big]^\top
\bm\psi_n^\mu(t)
\Big),
\]
对所有 \(\bm\psi_n^\mu(t)\) 做高斯积分，利用
\(\mathbb E_{\bm\psi}e^{i\bm a^\top\bm\psi}
=\exp(-\tfrac12 \bm a^\top\bm\Sigma \bm a)\)，
得到一个关于 \(\bm w_n,\hat{\bm w}_n,\Delta,\hat\Delta\) 的有效作用量 \(S\)，其中耦合项通过 \(\bm\Sigma(t,t')\) 体现时间相关性。这里的计算与 [PAPER 1, App.~A.1, Eq.(A.6–A.8)] 完全平行。

**物理含义：**  
这一步完成了从“具体轨迹上的特征随机性”到“只通过协方差 \(\bm\Sigma(t,t')\) 进入”的转换，是 DMFT 的关键。TD 误差与权重之间的复杂耦合被写成了二次型。

---

### 序参量与鞍点方程

为了在 \(N,B\to\infty\) 极限下得到可解的理论，需要引入一组宏观序参量（order parameters）来描述系统状态，这与自旋玻璃 / 神经网络 DMFT 完全一致。

#### 定义序参量

仿照 [PAPER 1, App.~A.1]，定义：

1. **TD 误差相关函数：**
\[
Q_n(t,t')
=
\frac{1}{B}\sum_{\mu=1}^B
\Delta_n^\mu(t)\Delta_n^\mu(t').
\tag{1.7}
\]
物理意义：在第 \(n\) 次迭代时，同一 episode 内不同时刻 TD 误差的二阶相关，是“噪声强度”的序参量。

2. **权重–特征二阶矩（value 自相关）：**
\[
C_n(t,t')
=
\frac{1}{N}\bm w_n^\top \bm\Sigma(t,t')\bm w_n.
\tag{1.8}
\]
物理意义：在一个“典型 episode”上，用当前权重 \(\bm w_n\) 预测时刻 \(t,t'\) 的值 \(\hat V_n(s_t),\hat V_n(s_{t'})\) 的协方差。

3. **reward–value 交叉相关：**
\[
C_n^R(t,t')
=
\frac{1}{N}\bm w_R^\top \bm\Sigma(t,t')\bm w_n.
\tag{1.9}
\]
物理意义：奖励方向 \(\bm w_R\) 与当前值估计方向 \(\bm w_n\) 在特征空间的对齐程度。

4. **响应相关序参量（conjugate fields）：**
\[
D_n(t,t')
= -\frac{i}{N}\hat{\bm w}_n^\top \bm\Sigma(t,t')\bm w_n,
\quad
D_n^R(t,t')
= -\frac{i}{N}\hat{\bm w}_n^\top \bm\Sigma(t,t')\bm w_R.
\tag{1.10}
\]
物理意义：这些是伴随变量 \(\hat{\bm w}_n\) 与 \(\bm w_n\) 的耦合，用于提取响应函数（类似 Martin–Siggia–Rose 形式）。

随后，通过在路径积分中插入 delta 约束，将这些宏观序参量“提升”为积分变量。以 \(Q_n(t,t')\) 为例：
\[
1
=
\int dQ_n(t,t')\,
\delta\Big(
B Q_n(t,t') - \sum_\mu\Delta_n^\mu(t)\Delta_n^\mu(t')
\Big)
\]
\[
=
\int\frac{dQ_n d\hat Q_n}{4\pi i}
\exp\Big\{
\frac{B}{2}\hat Q_n Q_n
-\frac12\hat Q_n \sum_\mu\Delta_n^\mu(t)\Delta_n^\mu(t')
\Big\}.
\tag{1.11}
\]
对其它序参量 \(C,C^R,D,D^R\) 也做同样处理（[PAPER 1, App.~A.1, Eq.(A.9–A.10)]）。

#### 有效作用量与鞍点方程

引入所有序参量及其共轭量 \(\hat q\) 后，生成泛函可以写成
\[
Z[\{\bm j_n\}]
\propto
\int \mathcal D q\,
\exp\left(
\frac N2 S[q;\{\bm j_n\}]
\right),
\tag{1.12}
\]
其中 \(q=\{Q,\hat Q,C,\hat C,C^R,\hat C^R,D,\hat D,D^R,\hat D^R\}\)，而
\[
S = S_{\text{macro}}(q) + S_w(q;\{\bm j_n\}) + S_\Delta(q),
\]
具体形式与 [PAPER 1, App.~A.1, Eq.(A.11–A.13)] 完全一致。这一步的关键是：

- 所有的 \(\bm\psi\) 已经被高斯积分消去，只剩下通过 \(\bm\Sigma(t,t')\) 出现；  
- 系统的自由能按 \(N\) 比例放大，因此在 \(N\to\infty\) 时，自平均成立，积分由鞍点主导。

**鞍点条件.**  
在 \(N\to\infty\) 且 \(B\to\infty\) 但 \(\alpha=B/N=\mathcal O(1)\) 的比例极限下，用鞍点近似（saddle-point approximation）：
\[
\frac{\partial S}{\partial q}=0,
\tag{1.13}
\]
得到一组关于序参量的自洽方程。这些方程在 [PAPER 1, App.~A.1, Eq.(A.14–A.22)] 中被逐项写出。形式上，它们都形如：
\[
Q_n(t,t') = \langle \Delta_n(t)\Delta_n(t')\rangle_{\text{eff}},
\quad
C_n(t,t') = \frac1N\langle \bm w_n^\top\bm\Sigma(t,t')\bm w_n\rangle_{\text{eff}},
\]
等等，其中 \(\langle\cdot\rangle_{\text{eff}}\) 是后面将出现的“有效单元过程”的平均。

> 【说明】  
> - 这里“利用鞍点近似”：假定 \(N\) 大，作用量 \(S\) 随 \(N\) 线性放大，所以主导贡献来自极值点，忽略 \(\mathcal O(1/N)\) 的涨落。  
> - 这是 DMFT / 统计物理中标准的自平均假设。

---

### HS 变换与闭合形式

到目前为止，\(\bm w_n\) 向量的各个坐标在作用量中仍然耦合。为了得到一个“单参数”的有效随机过程，需要将这些耦合“解耦”。这一步通过 **Hubbard–Stratonovich (HS) 变换** 完成。

#### HS 变换（解耦特征坐标）

例如，在 \(S_w\) 中会出现
\[
\exp\Big[
- \frac{\eta^2}{2T^2}
\sum_{n,tt'} Q_n(t,t')\,\hat{\bm w}_n^\top
\bm\Sigma(t,t')\hat{\bm w}_n
\Big],
\]
这是关于 \(\hat{\bm w}_n\) 的二次型。应用 HS 变换（“利用 HS 变换”）：
\[
\exp\Big[-\tfrac12\hat{\bm w}^\top\bm K \hat{\bm w}\Big]
=
\mathbb E_{\bm u\sim\mathcal N(0,\bm K)}
\exp\big(i\bm u^\top\hat{\bm w}\big),
\tag{1.14}
\]
我们可以用一个高斯噪声 \(\bm u_n^w\) 来替换这个二次项，使不同坐标 \(w_{n,i}\) 之间的耦合变成“通过共享噪声的耦合”。

同理，对 \(\Delta\) 相关项应用 HS 变换，引入标量噪声 \(u_n^\Delta(t)\)。完成所有 HS 变换后，可以对共轭变量 \(\hat{\bm w}_n,\hat\Delta_n(t)\) 做高斯积分，它们产生 \(\delta\)-函数约束，从而得到一个**等价的有效随机过程**（[PAPER 1, App.~A.1, Eq.(A.23–A.28)]）：
\[
\begin{cases}
\bm w_{n+1}
= \bm w_n + \bm u_n^w + \text{确定性漂移项},\\[3pt]
\Delta_n(t) = u_n^\Delta(t),
\end{cases}
\tag{1.15}
\]
其中
\[
\bm u_n^w
\sim\mathcal N\Big(
\bm 0,\,
\frac{\eta^2}{T^2}\sum_{tt'}
Q_n(t,t')\bm\Sigma(t,t')
\Big),
\]
而 \(u_n^\Delta(t)\) 是协方差为 \(\bm Q_n\) 的高斯噪声。

这是 DMFT 的核心：在高维极限下，原本高度耦合的 \(N\)-维权重更新，可以用一个带**高斯有效噪声**的单体随机差分方程来描述，其噪声协方差由宏观序参量 \(Q_n,C_n,\dots\) 决定。

#### 有效单元动态与矩方程

对有效过程 (1.15) 做均值与协方差运算，可以得到闭合在 \(\langle\bm w_n\rangle,\bm M_n\) 与 \(Q_n(t,t')\) 上的递推。

定义时间平均协方差矩阵（[PAPER 1, Eq.(3)]）：
\[
\bar{\bm\Sigma}
=\frac1T\sum_{t=1}^T \bm\Sigma(t,t),
\quad
\bar{\bm\Sigma}_+
=\frac1T\sum_{t=1}^T \bm\Sigma(t,t+1),
\]
以及
\[
\bm A
\equiv
\bar{\bm\Sigma} - \gamma \bar{\bm\Sigma}_+.
\tag{1.16}
\]

**(1) 权重均值演化.**  
由 (1.15) 取期望，注意 \(\langle \bm u_n^w\rangle=0\)，得到（[PAPER 1, Eq.(A.31)]）
\[
\langle \bm w_{n+1}\rangle
=
\langle\bm w_n\rangle
+ \eta\,\bm A\big(\bm w_{TD}-\langle\bm w_n\rangle\big),
\tag{1.17}
\]
其中
\[
\bm w_{TD}
=
\bm A^{-1}\bar{\bm\Sigma}\bm w_R
\tag{1.18}
\]
是无限 batch / 梯度流极限的 TD 固定点（[PAPER 1, App.~B]）。

**(2) 权重协方差演化.**  
定义
\[
\bm M_n
= \Big\langle(\bm w_n-\bm w_{TD})
(\bm w_n-\bm w_{TD})^\top\Big\rangle,
\]
利用 (1.15) 并注意 \(\bm u_n^w\) 与 \(\bm w_n\) 独立，得到（[PAPER 1, Prop.2.1, Eq.(2–3)]）
\[
\bm M_{n+1}
=
(\bm I-\eta\bm A)\bm M_n(\bm I-\eta\bm A)^\top
+
\frac{\eta^2}{\alpha^2T^2}
\sum_{t,t'}Q_n(t,t')\bm\Sigma(t,t'),
\tag{1.19}
\]
其中 \(\alpha=B/N\)，且
\[
Q_n(t,t')
=
\langle\Delta_n(t)\Delta_n(t')\rangle
\]
需要用 \(\bm M_n,\langle\bm w_n\rangle\) 展开。用 [PAPER 1, Eq.(A.36)] 的结果，可写为
\[
\begin{aligned}
Q_n(t,t')
&=
\frac1N\Big\langle
(\bm w_R-\bm w_n)^\top
\bm\Sigma(t,t')
(\bm w_R-\bm w_n)
\Big\rangle
\\
&\quad
+\frac{\gamma}{N}\Big\langle
(\bm w_R-\bm w_n)^\top
\bm\Sigma(t,t'+1)\bm w_n
\Big\rangle
\\
&\quad
+\frac{\gamma}{N}\Big\langle
\bm w_n^\top\bm\Sigma(t+1,t')
(\bm w_R-\bm w_n)
\Big\rangle
\\
&\quad
+\frac{\gamma^2}{N}\Big\langle
\bm w_n^\top\bm\Sigma(t+1,t'+1)\bm w_n
\Big\rangle.
\end{aligned}
\tag{1.20}
\]
将 \(\bm w_n = \bm w_{TD} + (\bm w_n-\bm w_{TD})\) 展开，并用 \(\bm M_n\) 和 \(\langle\bm w_n\rangle\) 表示，即得到 [PAPER 1, App.~A.2, Eq.(A.37)] 的显式形式（略去冗长展开，可直接引用为“闭合表达式”）。

**(3) 损失表达式.**  
按 (1.2)，
\[
\mathcal L_n
=
\frac1N\mathrm{Tr}\big(\bar{\bm\Sigma}\,\bm M_n\big).
\tag{1.21}
\]

---

### 结果与讨论（on-policy DMFT 闭合）

**【命题 1（对应 [PAPER 1, Prop.2.1]）】**  
在极限 \(N,B\to\infty\) 且 \(\alpha=B/N=\mathcal O(1)\)，episode 长度 \(T=\mathcal O(1)\)，在高斯等价假设下，on-policy、固定策略、线性函数逼近 TD(0) 学习的“典型”值函数误差
\(\mathcal L_n = \langle (V-\hat V_n)^2\rangle\) 满足闭合方程：
\[
\boxed{
\begin{aligned}
\mathcal L_n
&=
\frac1N\mathrm{Tr}\big(\bar{\bm\Sigma}\,\bm M_n\big),
\\
\bm M_{n+1}
&=
(\bm I-\eta\bm A)\bm M_n(\bm I-\eta\bm A)^\top
+
\frac{\eta^2}{\alpha^2T^2}
\sum_{t,t'}Q_n(t,t')\bm\Sigma(t,t'),\\
\bm A
&=
\bar{\bm\Sigma}-\gamma\bar{\bm\Sigma}_+,
\quad
\bar{\bm\Sigma}
= \frac1T\sum_t\bm\Sigma(t,t),
\quad
\bar{\bm\Sigma}_+
= \frac1T\sum_t\bm\Sigma(t,t+1),
\end{aligned}}
\tag{1.22}
\]
其中
\[
Q_n(t,t')
=\langle\Delta_n(t)\Delta_n(t')\rangle
\]
的闭合表达式由 (1.20) 与
\(\bm M_n,\langle\bm w_n\rangle\) 给出（可直接引用 [PAPER 1, Eq.(A.37)]）。  

> 这些方程即 [PAPER 1, Eqs.(2–4)] 的完整不跳步推导版本。

**若干注解：**

1. **无限 batch 极限.**  
令 \(\alpha=B/N\to\infty\)（等价于 \(B\to\infty\)），(1.19) 中噪声项消失，TD 动力学等价于确定性的梯度流：
\[
\bm M_{n+1}
=
(\bm I-\eta\bm A)\bm M_n(\bm I-\eta\bm A)^\top,
\]
若 \(\bm A\) 正定，随 \(n\) 收敛到 \(\bm 0\)，即 \(\mathcal L_n\to 0\)。这是 [PAPER 1, App.~B] 中的“ODE 极限”。

2. **plateau 与噪声标度.**  
在有限 batch \(B\) 下，(1.19) 的噪声项导致 \(\bm M_n\) 存在非零不动点。对小 \(\eta\gamma^2/B\) 进行标度分析（利用 [PAPER 1, App.~A.3] 方法），得到
\[
\bm M_\infty
= \mathcal O\big(\tfrac{\eta\gamma^2}{B}\big),\quad
\mathcal L_\infty
=\mathcal O\big(\tfrac{\eta\gamma^2}{B}\big),
\]
这正是 [PAPER 1, Prop.3.3] 的 plateau 标度结果：plateau 随 \(B\) 增大或 \(\eta,\gamma\) 减小而下降。

3. **与 GG 环境的对应.**  
在 Goblets & Ghost toy 环境中：
- \(s\)：包含 agent/ghost 位置、墙、goblet 集合等的游戏状态；  
- \(R(s)\)：来自 goblet/ghost 的即时得分；  
- \(\bm\psi(s)\)：例如在你的代码中用的线性特征向量（如随机高斯、RBF over grid 等）。  
在 on-policy baseline 中，你固定一个策略（比如 teacher tabular Q 的贪心策略），用上述公式即可从环境生成的轨迹估计 \(\bm\Sigma(t,t')\)，代入 (1.22) 预测 TD 学习曲线。

---

## II. Off-policy + 固定策略 + 函数逼近 + 自举  
（可对应论文 3.2 小节：固定策略 off-policy 死亡三角的 DMFT）

这一节在任务 I 的基础上，改为**行为策略 \(\mu\)** 与**目标策略 \(\pi\)** 不同，但两者都固定不随时间变化；仍然是线性函数逼近 + TD(0) bootstrapping。我们重点刻画：off-policy 带来的**分布失配**如何进入 DMFT，并如何改变噪声项与收敛性质。

### 问题设定（固定目标策略 \(\pi\)，固定行为策略 \(\mu\)）

#### 两个策略与轨迹分布

- 目标策略（evaluation policy）：\(\pi(a\mid s)\)，我们希望学习 \(V^\pi(s)\)。  
- 行为策略（behavior policy）：\(\mu(a\mid s)\)，真实环境运行时采取该策略采样数据。

行为策略 \(\mu\) 与目标策略 \(\pi\) 固定且不同，诱导出两类轨迹分布：

- 行为轨迹分布 \(p_\mu(\tau)\)：真实采样来自此分布；  
- 目标轨迹分布 \(p_\pi(\tau)\)：定义我们要评估的值函数。

**off-policy TD(0) 更新.**  
为得到无偏估计，我们采用标准的 importance sampling 校正 TD(0)（例如 Sutton & Barto, 2018）：

- importance ratio：
\[
\rho_t
=
\rho(S_t,A_t)
= \frac{\pi(A_t\mid S_t)}{\mu(A_t\mid S_t)}.
\]
- TD 误差仍定义为
\[
\Delta_t = R(S_t) + \gamma\hat V(S_{t+1}) - \hat V(S_t),
\]
但更新用带权半梯度
\[
\bm w_{n+1}
=
\bm w_n
+ \frac{\eta}{TB}\sum_{\mu=1}^B\sum_{t=1}^T
\rho_{n}^\mu(t)\,\Delta_n^\mu(t)\,
\bm\psi_n^\mu(t),
\tag{2.1}
\]
其中 \((S_t,A_t)\) 是在 \(\mu\) 下采样的。

> 【说明】  
> - 不引入 \(\rho_t\) 的“naive off-policy TD”严格来说是 on-policy w.r.t. \(\mu\)，只是学习的是 \(V^\mu\) 而非 \(V^\pi\)。为了真正刻画“分布失配”导致的死亡三角，我们在理论上使用带 \(\rho\) 的 off-policy TD(0)。  
> - 在 GG 代码里，“teacher 提供经验，student 评估另一个策略”的情形可对应 \(\mu\) 为 teacher policy，\(\pi\) 为要评估的 student/target policy。

仍然假设 reward 可线性表示：
\[
R(s) = \bm w_R^\top\bm\psi(s),
\]
目标值函数
\[
V^\pi(s) = \bm w_{TD}^\top\bm\psi(s),
\]
这一次 \(\bm w_{TD}\) 由**目标策略的协方差结构**决定（见下）。

#### 两种协方差与分布失配

定义在**行为策略**与**目标策略**下的特征协方差（对应 [PAPER 1, Eq.(3)] 的推广）：
\[
\bm\Sigma_\mu(t,t')
=
\Big\langle
\bm\psi(s_t)\bm\psi(s_{t'})^\top
\Big\rangle_{\tau\sim p_\mu(\tau)},
\quad
\bm\Sigma_\pi(t,t')
=
\Big\langle
\bm\psi(s_t)\bm\psi(s_{t'})^\top
\Big\rangle_{\tau\sim p_\pi(\tau)}.
\tag{2.2}
\]

时间平均：
\[
\bar{\bm\Sigma}_\mu = \tfrac1T\sum_t\bm\Sigma_\mu(t,t),\quad
\bar{\bm\Sigma}_{\mu,+} = \tfrac1T\sum_t\bm\Sigma_\mu(t,t+1),
\]
\[
\bar{\bm\Sigma}_\pi = \tfrac1T\sum_t\bm\Sigma_\pi(t,t),\quad
\bar{\bm\Sigma}_{\pi,+} = \tfrac1T\sum_t\bm\Sigma_\pi(t,t+1),
\]
并定义
\[
\bm A_\pi
= \bar{\bm\Sigma}_\pi - \gamma\bar{\bm\Sigma}_{\pi,+}.
\tag{2.3}
\]
真实目标值函数对应的 TD 固定点为
\[
\bm w_{TD}
=
\bm A_\pi^{-1}\bar{\bm\Sigma}_\pi\bm w_R,
\tag{2.4}
\]
与 on-policy 情况完全相同，只是用 \(\pi\) 下的协方差。

**分布失配的核心：**

- **漂移项（期望梯度）**：  
  若 \(\rho_t=\pi/\mu\) 完全正确，则
  \[
  \mathbb E_{\tau\sim p_\mu}[\rho_t g(S_t,A_t)] = \mathbb E_{\tau\sim p_\pi}[g(S_t,A_t)]
  \]
  对任意 \(g\) 成立。因此，在 **ODE 极限**（\(B\to\infty\)）下，off-policy TD 的**平均更新方向**与 on-policy TD w.r.t. \(\pi\) 一致，仍然收敛到 \(\bm w_{TD}\)。  
- **噪声协方差（有限 batch 效应）**：  
  有限 \(B\) 时，噪声项的协方差包含 \(\rho_t\rho_{t'}\) 的方差，高阶矩取决于 \(\mu,\pi\) 的差异，是死亡三角中 off-policy 造成不稳定的主要来源。

接下来我们在高斯等价与 DMFT 框架内精确写出这些噪声项。

---

### 高斯等价与矩生成函数（含 importance ratio）

#### 广义高斯等价：\(\rho_t\bm\psi_t\) 的处理

在 (2.1) 中，更新使用的是“带权特征”
\[
\bm\phi(t) = \rho_t \bm\psi(s_t).
\]
在一般 MDP 中，\(\rho_t\) 依赖于 \(S_t,A_t\)，与 \(\bm\psi(s_t)\) 有相关性。为获得可解的理论，我们做一个**额外简化假设**：

> 【假设（独立因子近似）】  
> 在高维极限下，importance ratio \(\rho_t\) 与特征方向 \(\bm\psi(s_t)\) 的相关性可以忽略，即  
> \(\rho_t\) 的统计仅通过其低阶矩（如 \(\langle\rho_t\rho_{t'}\rangle\)）进入，而与 \(\bm\psi(s_t)\) 解耦。  

在这一假设下，“利用广义高斯等价假设”，我们可以把 \(\bm\phi(t)\) 也近似为零均值高斯向量：
\[
\bm\phi(t)
\sim
\mathcal N\big(0,\ \bm\Sigma_{\text{eff}}(t,t')\big),
\quad
\bm\Sigma_{\text{eff}}(t,t')
=
\kappa(t,t')\bm\Sigma_\mu(t,t'),
\]
其中
\[
\kappa(t,t')
=
\mathbb E_{\tau\sim p_\mu}
[\rho_t\rho_{t'}].
\tag{2.5}
\]
- on-policy 极限：当 \(\mu=\pi\) 时，\(\rho_t\equiv 1\Rightarrow\kappa(t,t')=1\)，\(\bm\Sigma_{\text{eff}}=\bm\Sigma_\mu=\bm\Sigma_\pi\)，退化回 I 节情形；  
- 强 off-policy 时，\(\mathrm{Var}(\rho_t)\) 很大，\(\kappa(t,t)\gg 1\)，有效特征协方差在噪声项前被放大。

#### 矩生成函数与特征平均

构造与 I 节类似的生成泛函
\[
Z[\{\bm j_n\}]
=
\mathbb E_{\{\bm\psi,\rho\}}
\exp\Big(i\sum_n\bm j_n\cdot\bm w_n\Big),
\tag{2.6}
\]
其中现在的动力学约束是
\[
\bm w_{n+1}
=
\bm w_n
+
\frac{\eta}{TB}\sum_{\mu,t}
\rho_n^\mu(t)\Delta_n^\mu(t)\bm\psi_n^\mu(t).
\]
用相同的 delta + 傅里叶技巧引入 \(\Delta,\hat\Delta,\hat{\bm w}\)，对 \(\bm\phi(t)=\rho_t\bm\psi(t)\) 按高斯过程（协方差 \(\bm\Sigma_{\text{eff}}\)）做平均。得到的作用量结构仍然与 I 节完全平行，只是：

- 所有与更新噪声相关的 \(\bm\Sigma(t,t')\) 被 \(\bm\Sigma_{\text{eff}}(t,t')\) 取代；  
- 与 TD 固定点（真实值函数）相关的量仍由 \(\bm\Sigma_\pi(t,t')\) 决定。

这一区分在后面的闭合方程中体现：**漂移项用 \(\pi\) 的协方差，噪声项用 \(\mu\) 的协方差乘以 \(\kappa(t,t')\)**。

生成泛函形式仍为
\[
Z \propto \int \mathcal D q
\exp\Big(\frac N2 S[q]\Big),
\]
只是 \(S[q]\) 中出现了两套协方差 \(\bm\Sigma_\pi,\bm\Sigma_\mu\) 以及 \(\kappa(t,t')\)。

---

### 序参量与鞍点方程（含 off-policy 序参量）

序参量集合基本沿用 I 节：  
\[
Q_n(t,t') = \frac1B\sum_\mu \Delta_n^\mu(t)\Delta_n^\mu(t'),
\]
\[
C_n(t,t') = \frac1N \bm w_n^\top\bm\Sigma_\pi(t,t')\bm w_n,\quad
C_n^R(t,t') = \frac1N \bm w_R^\top\bm\Sigma_\pi(t,t')\bm w_n,
\]
再加上基于 \(\mu\) 的统计量：

> **新增序参量：行为分布下的 value 自相关**
> \[
> C^{(\mu)}_n(t,t')
> =
> \frac1N\bm w_n^\top\bm\Sigma_\mu(t,t')\bm w_n.
> \tag{2.7}
> \]
> 它直接捕捉“行为分布”下的特征方差，与噪声项强度有关。

以及原有的响应相关序参量 \(D,D^R\)（此处略写）。

鞍点条件 \(\partial S/\partial q=0\) 给出类似的自洽关系：
\[
Q_n(t,t') = \langle\Delta_n(t)\Delta_n(t')\rangle_{\text{eff}},
\]
\[
C_n(t,t') = \tfrac1N\langle\bm w_n^\top\bm\Sigma_\pi(t,t')\bm w_n\rangle,
\quad
C^{(\mu)}_n(t,t')
= \tfrac1N\langle\bm w_n^\top\bm\Sigma_\mu(t,t')\bm w_n\rangle,
\]
等等。这里的平均是对后面得到的有效单元随机过程。

---

### HS 变换与闭合形式（off-policy 固定策略）

与 I 节一样，对关于 \(\hat{\bm w}_n,\hat\Delta_n(t)\) 的二次项做 HS 变换，引入高斯噪声 \(\bm u_n^w,u_n^\Delta(t)\)，并对共轭变量积分。得到的有效过程与 (1.15) 完全同形，只是噪声协方差中出现了 \(\bm\Sigma_{\text{eff}}(t,t')\)：

\[
\bm u_n^w
\sim
\mathcal N\Big(
\bm 0,\,
\frac{\eta^2}{T^2}
\sum_{t,t'}
Q_n(t,t')\,
\bm\Sigma_{\text{eff}}(t,t')
\Big),
\tag{2.8}
\]
而 TD 固定点仍由 \(\bm A_\pi\) 决定（见 (2.3)-(2.4)）。

对该有效过程取均值与协方差，得到：

**(1) 均值演化（仍是 w.r.t. 目标策略 \(\pi\)）：**
\[
\langle\bm w_{n+1}\rangle
=
\langle\bm w_n\rangle
+
\eta\,\bm A_\pi\big(\bm w_{TD}-\langle\bm w_n\rangle\big),
\tag{2.9}
\]
这里使用了 importance sampling 的无偏性：
\[
\mathbb E_{\tau\sim p_\mu}
[\rho_t\,\Delta_t\,\bm\psi_t]
=
\mathbb E_{\tau\sim p_\pi}
[\Delta_t^{(\pi)}\,\bm\psi_t],
\]
从而漂移矩阵与 I 节完全相同，只是使用 \(\pi\) 下的协方差。

**(2) 协方差演化：**
\[
\boxed{
\begin{aligned}
\bm M_{n+1}
&=
(\bm I-\eta\bm A_\pi)
\bm M_n
(\bm I-\eta\bm A_\pi)^\top
\\
&\quad+
\frac{\eta^2}{\alpha^2T^2}
\sum_{t,t'}
Q_n(t,t')\,
\bm\Sigma_{\text{eff}}(t,t'),
\end{aligned}}
\tag{2.10}
\]
其中
\[
\bm\Sigma_{\text{eff}}(t,t')
=
\kappa(t,t')\bm\Sigma_\mu(t,t'),
\quad
\kappa(t,t')=\mathbb E_{\mu}[\rho_t\rho_{t'}].
\tag{2.11}
\]

**(3) TD 误差相关函数 \(Q_n(t,t')\).**  
与 I 节一样，\(Q_n\) 仍由
\[
Q_n(t,t')
=\langle\Delta_n(t)\Delta_n(t')\rangle
\]
给出，但 \(\Delta_n(t)\) 用目标策略的 Bellman 结构展开：
\[
\Delta_n(t)
=
\bm w_R^\top\bm\psi_t
+ \gamma \bm w_n^\top\bm\psi_{t+1}
- \bm w_n^\top\bm\psi_t,
\]
其一、二阶矩依赖于 \(\bm\Sigma_\pi(t,t')\) 以及
\(\bm M_n,\langle\bm w_n\rangle\)。因此 \(Q_n\) 的表达式与 (1.20) 相同，只是所有 \(\bm\Sigma\) 换成 \(\bm\Sigma_\pi\)（即在目标分布下求协方差）。

**(4) 值函数误差.**  
因为我们关心 \(V^\pi\) 的误差，自然采用 \(\pi\) 下的时间平均协方差：
\[
\mathcal L_n
=
\frac1N\mathrm{Tr}\big(
\bar{\bm\Sigma}_\pi\,\bm M_n
\big).
\tag{2.12}
\]

---

### 结果与讨论（off-policy 固定策略的死亡三角）

综上，在 off-policy + 固定策略下，我们得到如下**DMFT 闭合方程组**：

\[
\boxed{
\begin{aligned}
\mathcal L_n
&= \frac1N\mathrm{Tr}\big(\bar{\bm\Sigma}_\pi\,\bm M_n\big),\\
\bm M_{n+1}
&=
(\bm I-\eta\bm A_\pi)
\bm M_n
(\bm I-\eta\bm A_\pi)^\top
+
\frac{\eta^2}{\alpha^2T^2}
\sum_{t,t'}
Q_n(t,t')\,
\bm\Sigma_{\text{eff}}(t,t'),\\
\bm A_\pi
&=
\bar{\bm\Sigma}_\pi - \gamma\bar{\bm\Sigma}_{\pi,+},\\
\bm\Sigma_{\text{eff}}(t,t')
&=
\kappa(t,t')\bm\Sigma_\mu(t,t'),
\quad \kappa(t,t')=\mathbb E_{\mu}[\rho_t\rho_{t'}],\\
Q_n(t,t')
&=
\text{同 (1.20)，将所有 }\bm\Sigma\text{替换为 }\bm\Sigma_\pi.
\end{aligned}}
\tag{2.13}
\]

**与 on-policy 结果的关系：**

- 当 \(\mu=\pi\) 时，\(\rho_t\equiv 1\Rightarrow\kappa(t,t')=1,\ \bm\Sigma_\mu=\bm\Sigma_\pi\)，立刻退化为 I 节的 (1.22)。  
- 当 \(\mu\neq\pi\) 时：
  - **漂移矩阵 \(\bm A_\pi\)** 与目标策略相关，理论上保证（在 ODE 极限）收敛到 \(V^\pi\)；  
  - **噪声协方差** 则由行为协方差 \(\bm\Sigma_\mu\) 及 \(\kappa(t,t')\) 决定。若 \(\rho_t\) 的二阶矩很大（极端 off-policy），则噪声被放大，plateau 抬高，甚至破坏稳定性。

> 【Remark：off-policy 引入的不稳定源】  
> - 在死亡三角中，\(\gamma>0\)（自举），\(\bm\Sigma\) 非平凡（函数逼近），再加上 \(\mathrm{Var}(\rho_t)\) 很大（off-policy），(2.10) 中的噪声项可能远大于 on-policy 情形，引起权重协方差 \(\bm M_n\) 的爆炸。  
> - 这在 GG 环境中可通过让 teacher policy 与目标 policy 差异较大来观察：相同的 \(\eta,B,\gamma\) 下，off-policy 版本的 \(\mathcal L_n\) plateau 会更高，甚至不收敛。

---

## III. Off-policy + 非固定策略 + 函数逼近 + 自举  
（可对应论文 3.3 小节：Q-learning 驱动的非固定策略死亡三角）

这里引入最简单的**策略随时间更新**机制：基于当前 Q 函数的 \(\varepsilon\)-greedy 策略，仍然使用线性函数逼近 + TD(0) 自举（Q-learning 半梯度）。行为策略与目标策略在形式上相同（都由当前 Q 决定），但由于 Q-learning 的 bootstrapping 使用 \(\max_{a'}Q\)，在技术上仍属于 off-policy 更新。

我们在 DMFT 级别上同时跟踪：  
- 值函数参数 \(\bm w_n\) 的分布；  
- 由 \(\bm w_n\) 诱导的策略 \(\pi_n\) 及其引起的特征统计 \(\bm\Sigma_n(t,t')\)。

### 问题设定（Q-learning + ε-greedy 非固定策略）

#### 状态–动作特征与线性 Q 函数

现在考虑动作显式进入特征（与你的草稿记号保持一致）：
\[
\bm\phi(s,a)\in\mathbb R^N,
\quad
Q_{\bm w}(s,a) = \bm w^\top\bm\phi(s,a).
\]
在 GG 环境中，可以把 \(\bm\phi(s,a)\) 理解为对 \((s,a)\) 做 embedding 的线性特征，如随机高斯投影、one-hot + 随机矩阵等（见你的 `q_learning_linear.py`）。

奖励函数假设可线性分解（teacher–student 设定）：
\[
R(s,a) = \bm w_R^\top\bm\phi(s,a),
\]
真实最优 Q 函数
\[
Q^*(s,a) = \bm w_*^\top\bm\phi(s,a),
\]
在 tabular teacher 下可认为 \(\bm w_*\) 固定，但学生仅通过 \(\bm\phi\) 看到近似。

#### 策略更新：ε-greedy

在第 \(n\) 次迭代，给定当前参数 \(\bm w_n\)，定义学生策略（也是行为策略）：
\[
\pi_n(a\mid s)
=
\begin{cases}
1-\varepsilon, & a = \arg\max_{a'}Q_{\bm w_n}(s,a'),
\\
\varepsilon/(|\mathcal A|-1), & \text{otherwise}.
\end{cases}
\tag{3.1}
\]
这里 \(\varepsilon\) 固定，\(\pi_n\) 是 \(\bm w_n\) 的确定函数（打破极大值平局时可采用任意固定 tie-breaking）。

> 【说明】  
> - 这是 proposal 中提到的“最简单的 Q-learning 策略更新机制”；  
> - 在 GG toy 中，“student policy 由自身 Q 决定”就是这个形式；  
> - 虽然行为策略=目标策略（就行为分布而言是 on-policy），但 Q-learning 使用 \(\max_{a'}Q\) bootstrapping，对应评估的是“当前 greedy 策略”的值，因此从 Bellman 角度看仍是 off-policy。

#### Q-learning 半梯度更新

在第 \(n\) 次迭代，从策略 \(\pi_n\) 诱导的轨迹分布 \(p_{\pi_n}(\tau)\) 中抽取 \(B\) 条 episode：
\[
\tau_n^\mu = \{(s_n^\mu(t),a_n^\mu(t))\}_{t=1}^T,\quad
a_n^\mu(t)\sim\pi_n(\cdot\mid s_n^\mu(t)).
\]

对每个 transition \((s_t,a_t,s_{t+1})\)，定义 Q-learning TD 误差（见你草稿第 2 节）：
\[
\delta_n^\mu(t)
=
R\big(s_n^\mu(t),a_n^\mu(t)\big)
+
\gamma\max_{a'}Q_{\bm w_n}\big(s_n^\mu(t+1),a'\big)
-
Q_{\bm w_n}\big(s_n^\mu(t),a_n^\mu(t)\big),
\tag{3.2}
\]
半梯度更新为
\[
\bm w_{n+1}
=
\bm w_n
+
\frac{\eta}{TB}
\sum_{\mu=1}^B\sum_{t=1}^T
\delta_n^\mu(t)\,
\bm\phi\big(s_n^\mu(t),a_n^\mu(t)\big).
\tag{3.3}
\]
这正是你在附录中推导的“semi-gradient Q-learning update”（参见草稿附录 \(\S\)TD gradient and update rules）。

> 【说明（死亡三角三要素）：**
> - **函数逼近**：\(\bm\phi(s,a)\) 共享参数；  
> - **自举**：\(\max_{a'}Q_{\bm w_n}(s_{t+1},a')\)；  
> - **off-policy**：TD 目标对应于“greedy 策略”的 Bellman 方程，而行为策略是 \(\varepsilon\)-greedy，有 exploration；再加上 function approximation，组合成经典 deadly triad。

我们的目标是在 DMFT 框架下同时刻画：

1. 在给定策略 \(\pi_n\) 的“快时间”尺度上，Q-learning 的 TD 噪声如何作用在 \(\bm w_n\) 上；  
2. 在“慢时间”上，\(\bm w_n\) 的变化如何经由 (3.1) 反过来改变 \(\pi_n\) 和特征协方差 \(\bm\Sigma_n(t,t')\)。

---

### 高斯等价与矩生成函数（策略随时间更新）

#### 条件高斯等价：给定策略 \(\pi_n\)

在固定策略 \(\pi_n\) 下，状态–动作对 \((S_t,A_t)\) 沿轨迹分布 \(p_{\pi_n}(\tau)\) 演化。定义此时的特征协方差：
\[
\bm\Sigma_n(t,t')
=
\Big\langle
\bm\phi\big(S_t,A_t\big)
\bm\phi\big(S_{t'},A_{t'}\big)^\top
\Big\rangle_{\tau\sim p_{\pi_n}(\tau)}.
\tag{3.4}
\]
这是一个**随迭代 \(n\) 演化**的协方差族。高斯等价假设推广为：

> 【高斯等价（随策略演化版）】  
> 在每个迭代 \(n\) 上，可以用一个零均值高斯过程
> \[
> \bm\phi_G^{(n)}(t)
> \sim \mathcal N\big(0,\bm\Sigma_n(t,t')\big)
> \]
> 替代真实的 \(\{\bm\phi(s_t,a_t)\}\)。  
> 协方差 \(\bm\Sigma_n\) 通过策略 \(\pi_n\) 及环境动态 \(P\) 决定。

在 GG 环境中，可以通过长时间 rollouts 估计 \(\bm\Sigma_n\)；在理论推导中，我们把 \(\bm\Sigma_n\) 看成由策略序参量决定的对象（见下）。

#### 矩生成函数：对轨迹与策略的联合平均

与 I、II 节不同，这里轨迹分布本身依赖于参数轨迹 \(\{\bm w_n\}\)，因为每次迭代的行为策略 \(\pi_n\) 由 \(\bm w_n\) 诱导。

形式上，我们可以写出完整的生成泛函：
\[
Z[\{\bm j_n\}]
=
\mathbb E_{\{\tau_n^\mu\},\{\bm w_n\}}
\exp\Big(i\sum_n\bm j_n\cdot\bm w_n\Big),
\tag{3.5}
\]
其中 \(\{\tau_n^\mu\}\) 的分布为
\[
\tau_n^\mu \sim p_{\pi_n}(\tau),
\quad
\pi_n = \Pi[\bm w_n],
\tag{3.6}
\]
\(\Pi\) 表示由 (3.1) 规定的从 \(\bm w_n\) 到策略的映射。

严格而言，这导致轨迹生成与参数更新强烈耦合。为了进入 DMFT，我们做如下两步：

1. **条件在给定策略轨迹 \(\{\pi_n\}\) 上**，对特征 \(\bm\phi\) 使用高斯等价，得到一个条件生成泛函 \(Z[\{\bm j_n\}\mid\{\pi_n\}]\)，其形式与 I 节完全平行，只是协方差换成 \(\bm\Sigma_n\)。  
2. 再对策略轨迹 \(\{\pi_n\}\) 做平均。由于 \(\pi_n\) 是 \(\bm w_n\) 的确定函数，这一步等价于对 \(\{\bm w_n\}\) 的分布求平均，并将 \(\bm\Sigma_n\) 看成 \(\bm w_n\) 的函数。

**第 1 步：固定 \(\{\pi_n\}\) 下的生成泛函.**  
重复 I 节的步骤：引入 \(\delta\)-约束与傅里叶共轭变量，平均掉高斯特征 \(\bm\phi\)，得到
\[
Z[\{\bm j_n\}\mid\{\pi_n\}]
\propto
\int\mathcal D q
\exp\Big(\frac N2 S[q;\{\bm\Sigma_n\},\{\bm j_n\}]\Big),
\tag{3.7}
\]
其中作用量结构与 I 节完全同形，只是所有 \(\bm\Sigma(t,t')\) 替换为 \(\bm\Sigma_n(t,t')\)。

**第 2 步：引入策略相关序参量，求鞍点.**  
由于 \(\bm\Sigma_n\) 依赖于 \(\pi_n\)，而 \(\pi_n\) 又依赖于 \(\bm w_n\)，我们需要引入新的序参量来刻画策略统计。

---

### 序参量与鞍点方程（增加策略相关序参量）

除了 I 节中的 \(Q_n,C_n,C_n^R,D_n,D_n^R\) 外，我们增加与策略 / 访问分布相关的宏观量。

#### Occupancy / 策略序参量

在有限 MDP 中，可以定义某个“行为占用度量”（state–action occupancy），例如在第 \(n\) 次迭代下的平均占用频率：
\[
d_n(s,a)
=
\lim_{T\to\infty}
\frac1T\sum_{t=1}^T
\mathbb P_{\tau\sim p_{\pi_n}(\tau)}
[S_t=s,A_t=a].
\tag{3.8}
\]
在 GG 环境中，\(d_n\) 可以理解为 agent 在网格上访问每个 (位置,动作) 的频率。

在高维极限下，我们不需要记录每个 \((s,a)\) 的 \(d_n\)，而是抽象出**对特征统计有贡献的有限个组合**。例如，可以定义

- **策略–特征协方差序参量：**
  \[
  \bm\Sigma_n(t,t')
 =
  \sum_{s,a}\sum_{s',a'}d_n^{(t,t')}(s,a;s',a')
  \,\bm\phi(s,a)\bm\phi(s',a')^\top,
  \tag{3.9}
  \]
  其中 \(d_n^{(t,t')}\) 是在时刻 \(t,t'\) 的联合占用概率。这在理论上完全等价于 (3.4) 中的定义，只是更显式地写出其对 \(\pi_n\) 的依赖。  

- **策略相关的少数“方向对齐”量**，如
  \[
  u_{n,k}
 =
  \frac1N\bm w_n^\top\bm\Sigma^{(k)}\bm w_*,
  \tag{3.10}
  \]
  其中 \(\{\bm\Sigma^{(k)}\}\) 是某个有限基底（例如对应不同 reward 结构、不同子空间），类似于 RL perceptron 文献中的“重叠”（overlap）参数。

在这篇工作里，我们抽象地把所有与策略相关的宏观统计统称为
\[
\bm m_n \equiv \mathcal M[\pi_n],
\]
并假设存在函数关系
\[
\bm\Sigma_n(t,t')
= \mathcal K\big(\bm m_n\big),
\tag{3.11}
\]
即特征协方差由策略相关序参量 \(\bm m_n\) 决定。

#### 鞍点条件：值函数与策略的耦合

在引入 \(\bm m_n\) 并将 \(\bm\Sigma_n\) 当作其函数后，作用量 \(S[q;\{\bm\Sigma_n\}]\) 取决于 \(\{\bm m_n\}\)。对所有序参量做鞍点：
\[
\frac{\partial S}{\partial q}=0,
\quad
\frac{\partial S}{\partial \bm m_n}=0,
\tag{3.12}
\]
得到两类自洽方程：

1. **给定 \(\bm\Sigma_n\) 时的“值函数 DMFT”方程**：  
   完全类似 I 节，只是所有协方差矩阵均加下标 \(n\)，例如
   \[
   \langle\bm w_{n+1}\rangle
   =
   \langle\bm w_n\rangle
   +
   \eta\,\bm A_n
   \big(\bm w_{TD}^{(n)}-\langle\bm w_n\rangle\big),
   \tag{3.13}
   \]
   其中
   \[
   \bm A_n
  =
   \bar{\bm\Sigma}_n - \gamma\bar{\bm\Sigma}_{n,+},
   \quad
   \bm w_{TD}^{(n)}
  =
   \bm A_n^{-1}\bar{\bm\Sigma}_n\bm w_R,
   \]
   \(\bm M_n\) 的演化类似 (1.19) 但协方差为 \(\bm\Sigma_n\)。  
   也就是说：**在一个“准静态”策略 \(\pi_n\) 下，值函数演化由与 I 节同构的 DMFT 描述。**

2. **策略序参量 \(\bm m_n\) 的演化方程**：  
   由 \(\partial S/\partial \bm m_n=0\) 得到 \(\bm m_{n+1}\) 与 \(\bm m_n, \langle\bm w_n\rangle,\bm M_n\) 的关系。形式上相当于一个“策略更新”的平均动力学：
   \[
   \bm m_{n+1}
   = \mathcal F\big(\bm m_n;\ \langle\bm w_n\rangle,\bm M_n\big),
   \tag{3.14}
   \]
   其中 \(\mathcal F\) 具体取决于 \(\varepsilon\)-greedy 规则 (3.1) 和环境转移结构。

在 Q-learning ε-greedy 情形，可以更直观地写成：

> - 给定 \(\bm w_n\)，每个状态 \(s\) 下的 greedy 动作是
>   \[
>   a^*_n(s) = \arg\max_{a}Q_{\bm w_n}(s,a).
>   \]
> - 策略 \(\pi_n\) 对应的占用度（例如 \(d_n(s,a)\)）由 \(\varepsilon\)-greedy 决定；  
> - 于是 \(\bm m_n = \mathcal M[\bm w_n]\)，\(\bm\Sigma_n = \mathcal K[\mathcal M(\bm w_n)]\)，成为 \(\bm w_n\) 的函数。  
> - 通过对 \(Q_{\bm w_n}(s,a)\) 的高斯近似，可以进一步将 \(\mathcal M\) 和 \(\mathcal K\) 显式写成 \(\langle\bm w_n\rangle,\bm M_n\) 的函数，实现**形式上的闭合**。

---

### HS 变换与闭合形式（联合值函数–策略 DMFT）

重复 I 节的 HS 步骤，对每个固定 \(n\) 的 \(\bm\Sigma_n\) 应用 HS 变换，得到**条件在策略上的**有效过程：
\[
\begin{cases}
\bm w_{n+1}
=
\bm w_n
+ \bm u_n^w
+ \text{确定性漂移项（依赖 }\bm\Sigma_n),\\[3pt]
\bm u_n^w
\sim\mathcal N\big(
\bm 0,\,
\frac{\eta^2}{T^2}
\sum_{tt'}Q_n(t,t')\bm\Sigma_n(t,t')
\big),
\end{cases}
\tag{3.15}
\]
然后通过 (3.11) 与 (3.14) 将 \(\bm\Sigma_n\) 用策略序参量 \(\bm m_n\) 表示，再用 \(\bm m_n=\mathcal M(\bm w_n)\) 关系将其写成 \(\bm w_n\) 分布的函数。

**最终得到一组“联立有效动力学”：**

- **值函数相关：**
  \[
  \boxed{
  \begin{aligned}
  \langle\bm w_{n+1}\rangle
  &=
  \langle\bm w_n\rangle
  +
  \eta\,\bm A_n
  \big(\bm w_{TD}^{(n)}-\langle\bm w_n\rangle\big),
  \\
  \bm M_{n+1}
  &=
  (\bm I-\eta\bm A_n)\bm M_n(\bm I-\eta\bm A_n)^\top
  +
  \frac{\eta^2}{\alpha^2T^2}
  \sum_{tt'}Q_n(t,t')\bm\Sigma_n(t,t'),
  \\
  \bm A_n
  &=
  \bar{\bm\Sigma}_n-\gamma\bar{\bm\Sigma}_{n,+},
  \quad
  \bm w_{TD}^{(n)}
  = \bm A_n^{-1}\bar{\bm\Sigma}_n\bm w_R,\\
  Q_n(t,t')
  &=
  \text{按 (1.20)，将 }\bm\Sigma\text{替换为 }\bm\Sigma_n.
  \end{aligned}}
  \tag{3.16}
  \]

- **策略相关：**
  \[
  \boxed{
  \bm m_{n+1}
  = \mathcal F\big(\bm m_n;\ \langle\bm w_n\rangle,\bm M_n\big),\qquad
  \bm\Sigma_{n+1} = \mathcal K(\bm m_{n+1}).
  }
  \tag{3.17}
  \]

值函数误差可写为
\[
\mathcal L_n
=
\frac1N\mathrm{Tr}\big(\bar{\bm\Sigma}_n\,\bm M_n\big),
\tag{3.18}
\]
注意这里的 \(\bar{\bm\Sigma}_n\) 也随策略演化。

---

### 结果与讨论（非固定策略死亡三角）

1. **与 II 节的关系（固定策略 off-policy）.**  
   若人为“冻结”策略更新，即令 \(\bm m_n\equiv \bm m_0\)，则 \(\bm\Sigma_n\equiv\bm\Sigma_0\)，(3.16) 立刻退化为 I/II 节的固定策略 DMFT。此时的 off-policy 仅体现在行为分布与要评估的 Bellman 方程（如目标 greedy 策略）不一致。

2. **策略更新引入的额外耦合.**  
   (3.16)-(3.17) 显示，策略相关序参量 \(\bm m_n\) 通过 \(\bm\Sigma_n\) 影响 TD 噪声与漂移；反过来 \(\bm w_n\) 的演化又决定 \(\bm m_{n+1}\)。这形成一个**闭合但高度非线性**的“值–策”耦合动力系统，是 DMFT 级别的 actor-like 行为。

3. **时间尺度问题.**  
   在这里我们没有强加时间尺度分离：策略更新与值函数更新在同一迭代步长下发生，因此 \(\bm\Sigma_n\) 变化可能很快，导致学习曲线更不稳定。你在 proposal 中提到的“two-time-scale actor–critic”（如 Konda–Tsitsiklis）对应在 (3.17) 中引入一个小步长 \(\eta_\pi\ll\eta\)，使 \(\bm m_n\) 变慢，这将在下一节（IV）通过 schedule 明确处理。

4. **与 GG 实验的对应.**  
   - 在 `q_learning_linear.py` 中，学生采用线性 Q + ε-greedy；  
   - GG 环境中的状态–动作分布随 Q-learning 进展不断偏向“高 Q”的区域；  
   - 这正对应 \(\bm\Sigma_n\) 随 \(n\) 演化；  
   - 你在实验中观察到的 “先下降再发散” 的损失曲线，可以在 (3.16)-(3.18) 的框架下理解为：  
     初期策略尚未“收缩”到某些危险子空间，plateau 可控；随着策略越来越集中在某些特征方向，自举 + 函数逼近在这些方向上的噪声被放大，导致 \(\bm M_n\) 发散。

---

## IV. Off-policy + 慢变策略 + 函数逼近 + 自举  
（可对应论文 3.4 小节：调度式慢变策略下的死亡三角 DMFT）

本节在 III 的基础上，引入 proposal 中提到的 **scheduling 慢变策略**：策略不再由 Q-learning 自由更新，而是按照预定调度缓慢变化，或者在值函数更新和策略更新之间引入明确的时间尺度分离。

目标：在 DMFT 级别上说明：  
- 当策略随时间变化很慢时，可以把它视为“准静态参数”；  
- TD DMFT 方程在每个时间块上近似等价于固定策略的结果；  
- 慢策略使系统更有希望停留在“稳定区域”，抑制死亡三角引发的发散。

### 问题设定（显式时间尺度分离与调度）

#### 调度式策略 schedule

假设有一个**外部给定的**策略 family \(\{\pi_\theta:\theta\in\Theta\}\)，我们让 \(\theta\) 随迭代缓慢变化：

- 引入“慢时间尺度” \(\tau_\pi\gg 1\)，并定义
  \[
  \theta_n = \theta\Big(\frac{n}{\tau_\pi}\Big),
  \]
  其中 \(\theta(u)\) 是随连续时间 \(u\) 缓慢变化的 schedule（例如线性插值、分段常数等）。  
- 在 GG 中，这对应例如“每隔 \(K\) 个 episode 更新一次行为策略”或“策略参数用小学习率 \(\eta_\pi\) 迭代更新，而值函数用大步长 \(\eta\)”。

在本节中，我们**不再让 \(\theta_n\) 由 \(\bm w_n\) 决定**，而是将其看作给定的外部控制变量。这样，策略序列 \(\{\pi_n\}\) 被 schedule 完全确定。

> 【区别于 III 节】  
> - III 节：\(\pi_n = \Pi[\bm w_n]\)（endogenous policy）；  
> - IV 节：\(\pi_n = \pi_{\theta_n}\)（exogenous schedule），但我们可以选择 \(\theta_n\) 依赖于上一阶段的性能评估等宏观量。

#### 特征协方差的块状准静态近似

对每个 \(n\)，定义
\[
\bm\Sigma_n(t,t')
=
\Big\langle
\bm\phi(S_t,A_t)\bm\phi(S_{t'},A_{t'})^\top
\Big\rangle_{\tau\sim p_{\pi_{\theta_n}}(\tau)}.
\]
由于 \(\theta_n\) 随 \(n\) 缓慢变化，在时间尺度 \(O(1)\) 的若干步内（即 \(n\in[n_0,n_0+\Delta n]\)，\(\Delta n\ll\tau_\pi\)），我们可以近似认为 \(\pi_{\theta_n}\) 不变：
\[
\bm\Sigma_n(t,t')\approx \bm\Sigma_{\theta_{n_0}}(t,t'),
\quad
n_0\le n\le n_0+\Delta n\ll\tau_\pi.
\tag{4.1}
\]
> 【利用时间尺度分离假设】  
> - “快时间”尺度：TD 更新（\(\bm w_n\) 演化）在时间步长 1 上进行；  
> - “慢时间”尺度：\(\theta_n\) 在尺度 \(\tau_\pi\) 上变化；  
> - 在 \(O(1)\) 的 TD 步数内，可以把策略视作常数，从而允许在每个时间块上使用固定策略 DMFT。

---

### 高斯等价与矩生成函数（块状准静态）

在给定一个时间块 \([n_0,n_0+\Delta n]\) 内，我们可以完全照搬 I 节的高斯等价与路径积分推导，只是将协方差矩阵替换为该块对应的 \(\bm\Sigma_{\theta_{n_0}}\)。因此，在每个块内我们有

\[
Z_{[n_0,n_0+\Delta n]}[\{\bm j_n\}\mid\theta_{n_0}]
\propto
\int\mathcal D q
\exp\Big(
\frac N2 S_{\theta_{n_0}}[q;\{\bm j_n\}]
\Big),
\tag{4.2}
\]
鞍点与 HS 变换步骤与 I 节完全相同，得到一个**条件在 \(\theta_{n_0}\) 上的有效过程**。

当我们跨越多个块时，只需在每个块内使用相应的 \(\bm\Sigma_{\theta_{n_0}}\)，并在块与块之间更新 \(\theta\)，相当于把 II/III 的“时间依赖 \(\bm\Sigma_n\)”显式分块化。

---

### 序参量与鞍点方程（快慢两级）

在每个块内，定义与 I 节相同的序参量 \(Q_n,C_n,\dots\)，只不过这些序参量在块内部视为**快变量**，在块之间视为**慢变量**。策略参数 \(\theta_n\) 则是显式的慢变量。

#### 快变量：每个块内的 TD DMFT

给定块起点 \(n_0\) 以及 \(\theta_{n_0}\)，在块内 \(n\in[n_0,n_0+\Delta n]\) 有：

\[
\begin{aligned}
\langle\bm w_{n+1}\rangle
&=
\langle\bm w_n\rangle
+
\eta\,\bm A(\theta_{n_0})
\Big(\bm w_{TD}(\theta_{n_0})
-
\langle\bm w_n\rangle\Big),
\\
\bm M_{n+1}
&=
\big(\bm I-\eta\bm A(\theta_{n_0})\big)
\bm M_n
\big(\bm I-\eta\bm A(\theta_{n_0})\big)^\top
\\
&\quad+
\frac{\eta^2}{\alpha^2T^2}
\sum_{t,t'}
Q_n(t,t';\theta_{n_0})
\bm\Sigma_{\theta_{n_0}}(t,t'),
\end{aligned}
\tag{4.3}
\]
其中
\[
\bm A(\theta)
=
\bar{\bm\Sigma}_\theta - \gamma\bar{\bm\Sigma}_{\theta,+},
\quad
\bm w_{TD}(\theta)
=
\bm A(\theta)^{-1}\bar{\bm\Sigma}_\theta\bm w_R.
\tag{4.4}
\]
这是 I 节公式 (1.17)-(1.19) 的\(\theta\)-参数版本。

若在每个块内迭代足够久，使得 \(\bm w_n\) 几乎到达该块的固定点 \(\bm w_{TD}(\theta_{n_0})\) 附近，我们可以近似认为
\[
\bm M_n \approx \bm M^{(\theta_{n_0})}_\infty,\quad
\mathcal L_n\approx\mathcal L_\infty(\theta_{n_0}),
\]
即 TD 动力学在当前策略下达到其 plateau。

#### 慢变量：策略 schedule 的更新

策略参数 \(\theta_n\) 按 schedule 更新，例如：
- 分段常数：每 \(\tau_\pi\) 步跳到新的 \(\theta\)；
- 或者连续时间近似：
  \[
  \theta_{n+1}
  =
  \theta_n + \eta_\pi\,g(\theta_n),
  \quad \eta_\pi\ll\eta.
  \]

在 DMFT 层面，这意味着：

> - 在快时间上，\(\bm w_n\) 和 \(\bm M_n\) 快速收敛到给定 \(\theta_n\) 下的（近似）固定点；  
> - 在慢时间上，\(\theta_n\) 演化，从而缓慢改变 \(\bm A(\theta)\)、\(\bm\Sigma_\theta\) 和 plateau 水平；  
> - 整体上可以视为一个**快–慢耦合 ODE 系统**：  
>   \[
>   \begin{cases}
>   \dot{\bm w}(\tau)
>   = -\bm A(\theta(\tau))\big(\bm w-\bm w_{TD}(\theta(\tau))\big)
>   + \text{噪声项},\\[3pt]
>   \dot\theta(\tau)
>   = \eta_\pi\,g(\theta(\tau)),
>   \end{cases}
>   \]
>   其中 \(\tau\) 是 rescaled 时间，噪声协方差由 \(\bm\Sigma_{\theta(\tau)}\) 给出。

---

### HS 变换后的闭合形式（慢时间上的有效方程）

综合上述，我们可以给出在慢变策略极限 \(\tau_\pi\to\infty\) 下的**有效 DMFT 闭合形式**：

1. **快时间尺度上的 DMFT：**  
   对每一个“几乎不变”的策略参数 \(\theta\)，有一组 on-policy DMFT 方程：
   \[
   \begin{aligned}
   \mathcal L_n^{(\theta)}
   &= \frac1N\mathrm{Tr}\big(\bar{\bm\Sigma}_\theta\,\bm M_n^{(\theta)}\big),
   \\
   \bm M_{n+1}^{(\theta)}
   &=
   (\bm I-\eta\bm A(\theta))
   \bm M_n^{(\theta)}
   (\bm I-\eta\bm A(\theta))^\top
   +
   \frac{\eta^2}{\alpha^2T^2}
   \sum_{t,t'}
   Q_n^{(\theta)}(t,t')\bm\Sigma_\theta(t,t'),
   \\
   Q_n^{(\theta)}(t,t')
   &=
   \text{按 (1.20)，用 }\bm\Sigma\to\bm\Sigma_\theta.
   \end{aligned}
   \tag{4.5}
   \]

2. **慢时间上的“有效方程”：**  
   在块内近似 \(\bm M_n^{(\theta)}\approx\bm M_\infty^{(\theta)}\) 后，我们可以定义一个仅依赖于 \(\theta\) 的 plateau 函数
   \[
   \mathcal L_\infty(\theta)
   =
   \frac1N\mathrm{Tr}\big(\bar{\bm\Sigma}_\theta\,\bm M_\infty^{(\theta)}\big),
   \tag{4.6}
   \]
   它由 (4.5) 的不动点给出（如同 I 节的 plateau 分析）。随着 \(\theta\) 按 schedule 演化，\(\mathcal L_\infty(\theta(\tau))\) 描述了**慢时间上的有效损失轨迹**。

3. **闭合系统：**
   \[
   \boxed{
   \begin{aligned}
   \text{快时间: }&
   \bm w_n^{(\theta)},\bm M_n^{(\theta)}
   \text{ 按 (4.5) 收敛到 }\bm M_\infty^{(\theta)},\\
   \text{慢时间: }&
   \theta_{k+1}=\theta_k + \eta_\pi\,g(\theta_k),\quad
   \mathcal L_k^{\text{eff}} = \mathcal L_\infty(\theta_k).
   \end{aligned}}
   \tag{4.7}
   \]

---

### 结果与讨论（慢变策略对死亡三角的调节）

1. **退化到 II/III 的极限.**  
   - 若 \(\tau_\pi\to\infty\) 且 \(\theta\) 在整个训练中保持常数，则完全退化为固定策略 DMFT（I/II 节）。  
   - 若移除时间尺度分离（\(\eta_\pi\sim\eta\)），则回到 III 节的“非固定策略”耦合 DMFT。

2. **为什么慢变策略有助于稳定？**  
   - I 节与 II 节显示，TD plateau 与 \(\eta,\gamma,B,\bm\Sigma\) 的函数形式大致为
     \(\mathcal L_\infty\sim\mathcal O(\eta\gamma^2/B)\times f(\text{特征谱})\)。  
   - 当策略突变较快时，\(\bm\Sigma_n\) 可能瞬间跃迁到一些“危险子空间”（例如与 reward 对齐差、特征谱 condition number 差的方向），使 plateau 突然升高，甚至超出稳定范围。  
   - 慢变 schedule 相当于在策略空间中**缓慢移动**，每一步只在“局部稳定域”内小幅改变 \(\bm\Sigma_\theta\)，允许 \(\bm w_n\) 在每个中间策略下都接近局部 plateau，而不会因为瞬时的极端 off-policy/distribution-shift 而爆炸。

3. **与 GG 实验设计的联系.**  
   - 你在 GG 环境中可以实现类似 schedule：例如先用一个保守的策略（如 teacher policy）采样，估计一个较安全的值函数，然后逐步向更激进的策略（如 greedy w.r.t. 当前 Q）插值。  
   - 在 DMFT 语言下，这对应 \(\theta\) 从“安全策略”所在区域缓慢移动到“高奖励但潜在不稳定”的区域。  
   - 通过比较不同 \(\tau_\pi\)（或 \(\eta_\pi\)）下的 \(\mathcal L_k^{\text{eff}}\) 轨迹，可以直接验证慢变策略在抑制死亡三角导致的发散方面的效果。

4. **与 Pehlevan 论文结构的关系.**  
   - 整个推导流程严格沿用 [PAPER 1] 的 DMFT 思路（高斯等价 → 路径积分 → 序参量 → 鞍点 → HS → 有效单元动态），只是把协方差 \(\bm\Sigma\) 提升为“随策略缓慢变化的外参”；  
   - 新增的假设主要是**时间尺度分离**与**块状准静态近似**，这些在统计物理中用于 adiabatic approximation 的技术非常常见；  
   - 因此本节可以看作对 Pehlevan 结果的一个自然延伸，从“固定策略”的 DMFT 推广到“慢变策略”的 DMFT。

---

以上四节分别给出了：

- I：固定策略 on-policy TD(0) 的完整 DMFT 推导（忠实复现 [PAPER 1]）；  
- II：在固定策略 off-policy（带 importance ratio）的设置下，明确写出分布失配如何通过 \(\kappa(t,t')\) 与 \(\bm\Sigma_\mu,\bm\Sigma_\pi\) 进入 DMFT 噪声项；  
- III：在 Q-learning 驱动的非固定策略下，构造了联立的值函数–策略序参量 DMFT 闭合形式；  
- IV：在 scheduling 慢变策略下，引入时间尺度分离，得到块状准静态的快–慢 DMFT，有助于解释如何通过慢变策略抑制死亡三角的不稳定性。

这些内容可以直接拆分成你论文主文的 3.1–3.4 四个技术子节，并与 GG toy 实验中的 on-policy baseline、off-policy 固定策略、Q-learning 非固定策略与 schedule 策略实验一一对应。