【任务背景】
我正在写一篇关于 **off-policy TD 学习的 DMFT 分析与“死亡三角”（bootstrapping + 函数逼近 + off-policy）的理论研究** 的论文。
当前论文草稿位于 `workspace/projects/Loss_Dynamics_in_Temporal_Differences_of_Reinforcement_Learning_with_Unfixed_Policy/paper/main.tex`，对应章节大致为“死亡三角下TD Learning玩具模型，没有做DMFT分析” 相关部分。
Proposal 中已经设定，我们希望：
- 以 Pehlevan 等人的工作《Loss Dynamics of Temporal Difference Reinforcement Learning》（下称 Pehlevan 论文）为出发点；
- 用 DMFT 的框架系统分析在不同策略假设下（on-policy / off-policy / 固定策略 / 非固定策略 / 慢变策略），TD 学习的损失动力学和收敛/发散行为；
- 重点刻画“死亡三角”条件下（bootstrapping + 函数逼近 + off-policy）的失稳机制，并与 Goblets & Ghost (GG) toy 环境中的数值实验相对照。

【当前文章位置 / 约束】
- 当前章节正在讨论：**在 DMFT 框架下，TD 学习在不同策略设定下（on-policy 固定策略、off-policy 固定策略、off-policy 非固定策略、off-policy 慢变策略）的损失动力学**。
- 上一节已经给出了 TD 更新规则、特征向量表示和基本的 RL 问题设定（state features、value function、TD 误差等），但尚未系统展开 DMFT 推导。
- 我希望你这次给出的四种情形的推导结果，将来可以直接拆成论文中的四个子小节（比如 3.1, 3.2, 3.3, 3.4），但你不需要帮我写整段论文，只用给出**清晰、完整、公式齐全的推导过程**和简短文字说明。

【本次推导的总目标】
请你紧密基于 **Pehlevan 论文《Loss Dynamics of Temporal Difference Reinforcement Learning》** 的 DMFT 推导思路，围绕以下四种设定，完整走一遍
> “问题设定 → 高斯等价 → 矩生成函数 → 序参量 → 鞍点 → HS 变换 → 闭合”
这套流程，并分别给出最终的闭合形式和讨论。

### 任务 1：on-policy + 固定策略 + (函数逼近 + 自举)

1. 严格按照 Pehlevan 论文中的设定，整理 on-policy、固定策略、带函数逼近与 bootstrapping 的 TD 学习问题：
   - 明确 TD 更新方程、特征分布、奖励结构；
   - 给出所有必要的记号和假设（包括高斯等价假设）。
2. 在高斯等价假设下，写出相应的 generating function / 矩生成函数，并清楚说明：
   - 哪些随机量（如特征、奖励、噪声）被平均；
   - 哪些是 order parameters。
3. 明确写出 DMFT 的序参量（例如 \(Q(t,t')\), \(R(t,t')\), 以及你认为必要的其它 order parameters），并解释它们在 TD 学习中的物理/统计意义。
4. 展开鞍点近似与 Hubbard–Stratonovich (HS) 变换，完整说明每一步的逻辑：
   - 如何从原始损失或能量函数出发；
   - 如何把耦合项“解耦”成单点有效过程；
   - 最终得到的 effective single-neuron / single-parameter 动力学。
5. 给出 on-policy + 固定策略情形下的最终 DMFT 闭合方程（或方程组），并明确标注哪些地方直接来源于 Pehlevan 论文（可以引用论文中的式子），这一部分请尽量保持“忠实复现”的风格。

> 要求：这一部分可以理解为对 Pehlevan 论文 DMFT 推导的“总结 + 不跳步重写”，尽量完整，而不是只写“见原文”。

---

### 任务 2：off-policy + 固定策略 + (函数逼近 + 自举) —— 固定策略死亡三角

在任务 1 的 on-policy 情形基础上，改为 off-policy + 固定策略，保持函数逼近与 bootstrapping，分析“固定策略死亡三角”下的 DMFT。

1. 从“问题设定”部分开始重新写一遍，说明什么地方与 on-policy 不同：
   - 行为策略与评估策略不一致时，状态分布/特征分布如何变化；
   - TD 误差中的期望是针对哪一种分布计算；
   - 需要引入哪些额外的符号来刻画 distribution shift。
2. 在高斯等价假设下重写 generating function / 矩生成函数：
   - 清晰标出 off-policy 带来的交叉项或额外相关性；
   - 明确说明这些项在 on-policy 下为什么不存在或退化。
3. 重新定义（或扩展）序参量集合，使之能够捕捉 off-policy 带来的影响：
   - 哪些 order parameters 与“分布失配”直接相关；
   - 是否需要区分“行为策略下的统计量”和“目标策略下的统计量”。
4. 在 saddle-point 和 HS 步骤中，重点讨论：
   - 哪些交叉项是死守不动、必须保留的；
   - 哪些高阶项可以在某些极限下（如大系统极限、弱 off-policy 偏移）被近似忽略；
   - 对每一种近似给出明确的物理/数学理由。
5. 最终给出 off-policy + 固定策略死亡三角下的 DMFT 闭合方程，并说明：
   - 它在什么极限下会简化回任务 1 的 on-policy 结果；
   - 哪些额外项可以被自然解释为 “deadly triad” 中 off-policy 引入的不稳定源。

> 要求：即便步骤与任务 1 十分相似，也不要写“同任务 1 类似”，请把完整推导和关键公式完整写出，并显式指出和 on-policy 情形的差异。
> 你要重新按照，问题设定 → 高斯等价 → 矩生成函数 → 序参量 → 鞍点 → HS 变换 → 闭合的思路走一遍，不能因为只修改了某个参数而在结果或者只在几个关键步骤中修改这些参数，就变成新的结果了。比如矩生成函数可能和第一问一致，但你仍然需要进行推导，而不能用“不再赘述”来进行搪塞。我需要真正完整的推导。

---

### 任务 3：off-policy + 非固定策略 + (函数逼近 + 自举) —— 非固定策略死亡三角

在任务 2 的基础上，考虑 **策略本身也在更新** 的情形，但我们暂时不引入 actor–critic 或单独的策略网络，而是采用 **最简单的 Q-learning 策略更新机制**（例如基于当前 Q 的 ε-greedy 策略），仍然处在 off-policy + 函数逼近 + bootstrapping 的“死亡三角”框架下。

1. 在“问题设定”部分明确：
   - 策略如何随时间 \(t\) 更新：可以抽象成参数化策略 \(\pi_\theta\)，其中 \(\theta\) 实际上由当前的 Q 函数（或其参数 \(w\)）诱导，例如通过 ε-greedy：
     $$
     \pi_\theta(a\mid s) \propto 
     \begin{cases}
     1 - \varepsilon, & a = \arg\max_{a'} Q_w(s, a') \\
     \varepsilon / (|\mathcal{A}| - 1), & \text{otherwise}
     \end{cases}
     $$
     并给出相应的 \(\theta\) / \(w\) 更新规则（基于 Q-learning TD 误差）。
   - 是否存在 teacher policy / behavior policy 与 student policy 的区分，以及在 GG 环境中你习惯的设定（例如 teacher 提供经验，student 的策略由自身 Q 决定，但仍然是 off-policy 更新）。
2. 重写 generating function / 矩生成函数：
   - 把策略参数（或者由 Q 诱导出的策略）的随机性或更新过程纳入整体平均；
   - 说明策略更新对状态分布的反作用是如何进入 DMFT 的：例如，行为策略的分布 \(\mu_t(s,a)\) 随 \(t\) 演化，会反过来影响特征的统计量。
3. 在序参量设计上：
   - 增加与策略参数相关的 order parameters（例如“由 Q 决定的策略”下的状态–动作分布矩、value–policy 的交叉项等）；
   - 说明这些新的序参量在物理/统计上的意义，特别是它们如何刻画“策略随时间改变”这一点。
4. 鞍点与 HS 步骤中，重点讨论：
   - 与任务 2（off-policy + 固定策略）相比，多出哪些额外的耦合项（例如不同时刻的策略耦合）；
   - 是否需要引入“多时间尺度”的假设（例如 value 的更新比策略快，或者反之），即便我们暂时还不进入完整的 actor–critic 架构，也可以在 Q-learning 的 toy 模型里讨论这种时间尺度分离。
5. 给出 off-policy + 非固定策略死亡三角下的 DMFT 闭合形式：
   - 给出一组联立的 effective dynamics（例如 value 相关的有效过程 + 由 value 诱导的策略相关有效过程），并明确它们之间的耦合关系；
   - 讨论在某些极限下（比如策略更新极慢，或 ε 极小等）如何退化回任务 2 的固定策略结果。

> 要求：这一部分要自成一体地重新走完“问题设定 → 高斯等价 → 矩生成 → 序参量 → 鞍点 → HS → 闭合”，禁止简单“在任务 2 基础上稍作修改”式的省略，要真正拆出来写，并且显式指出和死亡三角固定策略情形（任务 2）的差异。
> 同时，请不要在本任务中引入完整的 actor–critic 框架；这一块保留到后续“自动策略更新框架”的工作中。
> 你要重新按照，问题设定 → 高斯等价 → 矩生成函数 → 序参量 → 鞍点 → HS 变换 → 闭合的思路走一遍，不能因为只修改了某个参数而在结果或者只在几个关键步骤中修改这些参数，就变成新的结果了。比如矩生成函数可能和第一问一致，但你仍然需要进行推导，而不能用“不再赘述”来进行搪塞。我需要真正完整的推导。


---

### 任务 4：off-policy + 慢变策略 + (函数逼近 + 自举) —— scheduling 慢变策略死亡三角

在任务 3 的非固定策略基础上，引入 **proposal 中提到的 scheduling 慢变策略** 机制，即策略按预定调度缓慢变化，而不是完全自由学习。

1. 在“问题设定”中明确：
   - scheduling 的具体假设：例如策略参数按照某个时间尺度 \(\tau_{\pi}\) 缓慢变化，或者通过某种外部 schedule 控制；
   - 说明这种 schedule 在 GG 环境中的具体含义（比如每若干 episodes 调整一次 policy，或者价值更新与策略更新时间尺度之比）。
2. 在 generating function / 矩生成函数中：
   - 把慢变策略视为一种“准静态”参数，或者显式加入时间尺度分离的假设；
   - 讨论：在 \(\tau_{\pi} \to \infty\) 或 \(\tau_{\pi} \gg \tau_V\) 极限下，如何近似策略为“块状常数”的过程。
3. 在序参量和鞍点分析中：
   - 引入与时间尺度相关的 order parameters，或者把原有序参量拆分成快慢两级；
   - 在 HS 变换后展示如何得到“慢时间上的有效方程”。
4. 给出最终的 DMFT 闭合形式，并讨论：
   - 在慢变策略极限下，为什么更有希望抑制死亡三角导致的发散；
   - 该闭合形式如何与任务 2（固定策略）和任务 3（一般非固定策略）之间建立联系。
5. 尽量点明哪些部分是严格从 Pehlevan 论文结构延伸过来的，哪些是我们自己提出的新假设或者近似。

> 要求：这一部分不能简单说“把任务 3 的策略更新换成慢变 schedule 即可”，必须重新写出全部的完整推导和关键公式以及鞍点条件，让读者可以一眼看出“慢变策略”在 DMFT 级别到底改变了什么。
> 你要重新按照，问题设定 → 高斯等价 → 矩生成函数 → 序参量 → 鞍点 → HS 变换 → 闭合的思路走一遍，不能因为只修改了某个参数而在结果或者只在几个关键步骤中修改这些参数，就变成新的结果了。比如矩生成函数可能和第一问一致，但你仍然需要进行推导，而不能用“不再赘述”来进行搪塞。我需要真正完整的推导。

---

【希望优先参考的文献 / 代码】

- 论文：
  - **Pehlevan 论文**：Loss Dynamics of Temporal Difference Reinforcement Learning  
    —— 这是整个 DMFT 推导的基础，任务 1 尽量“照搬 + 不跳步重写”，后面三个任务在结构上也要尽量保持与该文一致。
  - 如有其它辅助文献（例如 DMFT 在监督学习中的经典推导、actor–critic 理论等），你可以在需要时引用，但主线要始终围绕 Pehlevan 的工作。

- 代码（背景信息，主要辅助你理解设定，不需要给出具体代码）：
  - 我有一个 Goblets & Ghost (GG) toy 环境的 TD 学习实验代码，包含 on-policy baseline、off-policy 固定策略 baseline 以及非固定策略（甚至慢变策略）的实验。
  - 你在推导时可以用文字类比这些实验设定，例如“teacher policy 固定、student 仅更新 value function” 或 “policy 参数按 schedule 缓慢更新”。

- 代码和实验设定（仅作为背景，不需要输出具体代码）：

  - Goblets & Ghouls (GG) toy 环境代码位于 `workspace/projects/Loss_Dynamics_in_Temporal_Differences_of_Reinforcement_Learning_with_Unfixed_Policy/gg_env/`。  
    `README.md` 中描述了一个带 agent / ghost / goblets / walls 的随机网格世界，以及 tabular Q-learning 训练流程；环境的动作转移使用四元组 \([a,b,c,d]\) 的“滑动内核”，默认配置为 `[0.7, 0.1, 0.1, 0.1]`，对应“执行期望动作 / 顺时针滑一格 / 反向 / 逆时针滑一格”的概率。
  - `gg_core` 提供 `GameState`, `Action`, `GGConfig`, `parse_config`, `run` 等接口，定义了状态空间（包括 agent 位置、ghost 位置、墙、goblet 集合等）以及环境的 `step` / `reset` 操作，你可以在推导中把 DMFT 里的“状态”“动作”“奖励”“转移”严格对齐到这个接口上。:contentReference[oaicite:4]{index=4}
  - `q_learning.py` 中实现了 tabular Q-learning 基线；`q_learning_linear.py` 中实现了线性函数逼近的半梯度 Q-learning toy 模型，其中：
    - teacher 端是一个保守的 tabular Q-learning，训练出近似 \(Q^\*\) 的 Q_table；
    - toy 端是一个带线性特征 \(\phi(s,a)\) 的学生模型，用同一环境、同一数据分布进行训练。  
    这两者在我们的 DMFT 分析中分别对应 “off-policy 固定策略” 的基线和 “off-policy + 函数逼近 + 自举”的死亡三角非固定策略分析。
  - 在推导任务 2–4 时，请尽量用文字把 DMFT 中的状态 \(s\)、动作 \(a\)、特征 \(\phi(s,a)\)、策略 \(\pi\) 与 GG 代码中的具体设定对应起来（例如：状态包含 agent/ghost 的格点位置，奖励来自 goblet 的正负值，行为策略可能由 teacher tabular Q 或 student 线性 Q 决定），但不要输出或修改具体代码。
    - 此外，我在 `gg_env/gg_env_summary.md` 中整理了一份“GG 环境 ↔ DMFT 记号”的详细对照表，
    你在推导时可以参考这份词汇表来保持符号和代码实现的一致性，但不需要在输出中逐字复述它。


【推导风格要求】

- 全程使用 LaTeX 公式，关键步骤用 `$$ ... $$` 单独成行。
- 每一种设定（任务 1–4）内部都要完整走一遍：
  > 问题设定 → 高斯等价 → 矩生成函数 → 序参量 → 鞍点 → HS 变换 → 闭合  
  不允许因为与前面类似就只写“同上”，关键公式必须重新写出、并明确指出差异。
  > 你要重新按照，问题设定 → 高斯等价 → 矩生成函数 → 序参量 → 鞍点 → HS 变换 → 闭合的思路走一遍，不能因为只修改了某个参数而在结果或者只在几个关键步骤中修改这些参数，就变成新的结果了。比如矩生成函数可能和第一问一致，但你仍然需要进行推导，而不能用“不再赘述”来进行搪塞。我需要真正完整的推导。
- 请用“问题重述 → 记号整理 → 关键步骤分段推导 → 结果与讨论”的结构组织每个任务。
- 每做一步近似（例如忽略高阶项、假设自平均、引入时间尺度分离等）必须有一句话解释为什么合理。
- 尽量和 `main.tex` 中已有的符号保持一致；如果你觉得目前文章里的符号选择不利于 DMFT 推导，可以提出修改建议，但要显式指出。

【输出格式】

- 请按以下大纲输出，方便我直接拆分成论文小节：
  - `## I. On-policy + 固定策略 + 函数逼近 + 自举`
  - `## II. Off-policy + 固定策略 + 函数逼近 + 自举`
  - `## III. Off-policy + 非固定策略 + 函数逼近 + 自举`
  - `## IV. Off-policy + 慢变策略 + 函数逼近 + 自举`
- 每一节内部用 `### 问题设定`、`### 高斯等价与矩生成函数`、`### 序参量与鞍点方程`、`### HS 变换与闭合形式`、`### 结果与讨论` 等小标题。
- 输出适合直接粘贴进论文草稿的 Markdown/LaTeX 混合内容。
- 如果你认为某一段可以写成论文中的 “Proposition”、“Lemma”、“Remark”，可以用文字标注（例如“【Remark】在 off-policy 极小偏移极限下，该方程退化为 ...”），但不需要帮我润色成正式论文段落。
