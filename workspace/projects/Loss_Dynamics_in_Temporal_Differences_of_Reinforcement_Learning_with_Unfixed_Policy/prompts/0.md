【任务背景】
我正在写一篇关于 [填：例如“off-policy TD 学习的 DMFT 分析”] 的论文。
当前论文草稿位于 workspace/projects/XXX/paper/main.tex，对应章节大致为 [例如“第 3.2 节”]。
Proposal 中已经设定了 [简单写下 proposal 中关键假设/目标]。

【当前文章位置 / 约束】
- 当前章节正在讨论：[...]（例如“固定策略 + off-policy 设定下的学习曲线”）。
- 上一节已经得到了：[...]（可以引用你文章里已有的公式编号，例如 `Eq.(3.5)`）。
- 我希望这次的推导结果 **可以直接放进论文的某一小节**，但你不需要帮我写整段论文，只用给出清晰的推导过程和简短文字说明。

【本次推导目标】
请你在 **固定策略 + off-policy** 的设定下，完成以下工作：
1. 参照 Bordelon 等人的 DMFT 推导框架，重新搭建问题设定和记号（state feature, TD 参数, policy 等）。
2. 在高斯等价假设下，写出对应的 generating function / 矩生成函数，并明确指出哪些随机量被平均掉。
3. 明确写出 order parameters（例如 Q(t,t'), R(t,t') 等）并说明每个的物理/统计意义。
4. 在 saddle-point 和 HS 变换步骤中，重点讨论：off-policy 带来的“分布失配”交叉项，是否可以被忽略，如果不能，应该如何近似或保留。
5. 给出一个最终的闭合方程（哪怕是近似的），并说明它在什么条件下可能简化为 on-policy 的结果。

【希望优先参考的文献 / 代码】
- 论文：
  - PAPER 1: Bordelon et al. (2020) [...]  —— 优先参考。
  - PAPER 2: [...]（你可以在这里写一些你对论文的理解）。
- 代码（如果相关）：
  - 我现在有一个 toy model 仿真代码（如 gg_core 中的 off-policy TD 实验），主要结构是：[...]。
  - 你可以用文字方式参考代码逻辑，例如“teacher policy 固定，student 仅更新 value function”。

【推导风格要求】
- 全程使用 LaTeX 公式，关键步骤用 `$$ ... $$` 单独成行。
- 请用“问题重述 → 记号整理 → 关键步骤分段推导 → 结果与讨论”的结构。
- 每做一步近似（例如忽略高阶项、假设自平均等）必须有一句话解释为什么合理。
- 尽量和 `main.tex` 中已有的符号保持一致，如果你发现符号冲突，请指出。

【输出格式】
- 输出适合直接粘贴进论文草稿的 Markdown/LaTeX 混合内容。
- 如果你认为某一段可以写成论文的“Proposition”、“Lemma”、“Remark”，可以用文字标注出来，但不需要帮我润色整段论文。

